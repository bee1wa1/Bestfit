# streamlit_best_fit_app.py
# ---
# Streamlit app to estimate best-fitting probability distributions
# Tabs:
# 1) Best fit given dataset (raw observations)
# 2) Best fit given histogram (binned data)
#
# Distributions included (at minimum):
# - Normal (2-parameter)
# - Lognormal (2-parameter, loc=0) and Lognormal (3-parameter)
# - Weibull (2-parameter, loc=0)
# - Weibull (3-parameter)
# - Gamma (2-parameter, loc=0)
# - Exponential (1-parameter, loc=0)
#
# Outputs:
# - Summary table with parameter estimates, log-likelihood, AIC, BIC, KS statistic & p-value
# - Ranked model list by AIC (lower is better)
# - Plotly chart: histogram / binned bars with overlaid fitted PDFs
#
# Notes:
# - For histogram input, the app accepts either midpoints+counts or left+right+counts.
# - Histogram is converted to a pseudo-sample by repeating bin midpoints (with optional downsampling if huge).
# - KS p-values are approximate when parameters are estimated from the data.

import io
import math
import textwrap
from typing import Dict, Tuple, List, Optional

import numpy as np
import pandas as pd
import streamlit as st
from scipy import stats
import plotly.graph_objects as go

st.set_page_config(page_title="Best-Fit Distributions", layout="wide")

# ---------------------------------------------
# Helpers
# ---------------------------------------------

def _parse_percentiles_dup(s: str) -> List[float]:
    try:
        vals = [float(x) for x in s.replace("%", "").split(",") if x.strip()]
        vals = [v for v in vals if 0 < v < 100]
        vals = sorted(set(round(v, 4) for v in vals))
        if not vals:
            return [1, 5, 10, 25, 50, 75, 90, 95, 99]
        return vals
    except Exception:
        return [1, 5, 10, 25, 50, 75, 90, 95, 99]

def _series_to_csv_bytes(s: pd.Series, sep: str = ",") -> bytes:
    return s.to_csv(index=False, header=["value"], sep=sep).encode()


def _series_to_txt_bytes(s: pd.Series) -> bytes:
    # One value per line (UTF-8)
    return ("\n".join(map(str, s.tolist())) + "\n").encode()


def _series_to_xlsx_bytes(s: pd.Series, sheet_name: str = "Data") -> bytes:
    import io
    output = io.BytesIO()
    df = pd.DataFrame({"value": s})
    with pd.ExcelWriter(output) as writer:
        df.to_excel(writer, sheet_name=sheet_name, index=False)
    return output.getvalue()

def _parse_percentiles(s: str) -> List[float]:
    try:
        vals = [float(x) for x in s.replace("%", "").split(",") if x.strip()]
        vals = [v for v in vals if 0 < v < 100]
        vals = sorted(set(round(v, 4) for v in vals))
        if not vals:
            return [1, 5, 10, 25, 50, 75, 90, 95, 99]
        return vals
    except Exception:
        return [1, 5, 10, 25, 50, 75, 90, 95, 99]

@st.cache_data(show_spinner=False)
def read_numeric_series(file_bytes: bytes, decimal: str = ".", sep: str = ",") -> pd.Series:
    """Read a CSV/TXT with one numeric column or a DataFrame with a named numeric column.
    Returns a flattened Series of numeric values, dropping NaNs.
    Tries a few fallbacks if the first attempt fails.
    """
    # Try pandas read_csv with user-provided sep & decimal.
    try:
        df = pd.read_csv(io.BytesIO(file_bytes), sep=sep, decimal=decimal)
    except Exception:
        # fallback: try whitespace
        df = pd.read_csv(io.BytesIO(file_bytes), sep=None, engine="python")

    if df.shape[1] == 1:
        s = df.iloc[:, 0]
    else:
        # try to find a likely numeric column
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if numeric_cols:
            s = df[numeric_cols[0]]
        else:
            # try to coerce first column
            s = pd.to_numeric(df.iloc[:, 0], errors="coerce")

    s = pd.to_numeric(s, errors="coerce").dropna()
    return s.reset_index(drop=True)

@st.cache_data(show_spinner=False)
def read_histogram(file_bytes: bytes, decimal: str = ".", sep: str = ",") -> pd.DataFrame:
    """Read a CSV/TXT representing a histogram.
    Acceptable formats:
      A) columns: value,count   (value is midpoint)
      B) columns: bin,count     (alias for value,count)
      C) columns: left,right,count
    Returns a DataFrame with columns: left, right, mid, width, count
    """
    try:
        df = pd.read_csv(io.BytesIO(file_bytes), sep=sep, decimal=decimal)
    except Exception:
        df = pd.read_csv(io.BytesIO(file_bytes), sep=None, engine="python")

    cols = [c.strip().lower() for c in df.columns]
    df.columns = cols

    if set(["value", "count"]).issubset(cols) or set(["bin", "count"]).issubset(cols):
        vcol = "value" if "value" in cols else "bin"
        out = pd.DataFrame({
            "mid": pd.to_numeric(df[vcol], errors="coerce"),
            "count": pd.to_numeric(df["count"], errors="coerce")
        }).dropna()
        # Infer width from gap between mids (robust median gap)
        if len(out) > 1:
            gaps = np.diff(np.sort(out["mid"].values))
            width = np.median(gaps)
            if not np.isfinite(width) or width <= 0:
                width = (out["mid"].max() - out["mid"].min()) / max(len(out) - 1, 1)
        else:
            width = 1.0
        out["width"] = width
        out["left"] = out["mid"] - width / 2
        out["right"] = out["mid"] + width / 2
        return out[["left", "right", "mid", "width", "count"]].reset_index(drop=True)

    elif set(["left", "right", "count"]).issubset(cols):
        out = pd.DataFrame({
            "left": pd.to_numeric(df["left"], errors="coerce"),
            "right": pd.to_numeric(df["right"], errors="coerce"),
            "count": pd.to_numeric(df["count"], errors="coerce"),
        }).dropna()
        out["mid"] = (out["left"] + out["right"]) / 2
        out["width"] = out["right"] - out["left"]
        out = out[out["width"] > 0]
        return out[["left", "right", "mid", "width", "count"]].reset_index(drop=True)

    else:
        raise ValueError(
            "Histogram file must have either (value,count)/(bin,count) or (left,right,count) columns."
        )


def aic(loglik: float, k_params: int) -> float:
    return 2 * k_params - 2 * loglik


def bic(loglik: float, k_params: int, n: int) -> float:
    return math.log(n) * k_params - 2 * loglik


# ---------------------------------------------
# Fitting wrappers
# ---------------------------------------------

# Each fitter returns a dict with fields:
# name, params (dict), frozen (scipy frozen dist), loglik, aic, bic, ks_stat, ks_p


def _loglik_from_pdf(dist_frozen, data: np.ndarray) -> float:
    ll = np.sum(dist_frozen.logpdf(data))
    return float(ll) if np.isfinite(ll) else -np.inf


def _ks_test(dist_frozen, data: np.ndarray) -> Tuple[float, float]:
    # Use one-sample KS; parameters estimated => p-values are approximate
    vals = np.sort(data)
    cdf_vals = dist_frozen.cdf(vals)
    stat = np.max(np.abs(np.arange(1, len(vals) + 1) / len(vals) - cdf_vals))
    # Approximate p-value with scipy's kstest against empirical CDF
    try:
        stat2, pval = stats.kstest(vals, dist_frozen.cdf)
        return float(stat2), float(pval)
    except Exception:
        return float(stat), float("nan")


def fit_normal(data: np.ndarray) -> Dict:
    mu, sigma = stats.norm.fit(data)
    frozen = stats.norm(loc=mu, scale=sigma)
    ll = _loglik_from_pdf(frozen, data)
    k = 2
    return {
        "name": "Normal (mu, sigma)",
        "params": {"mu": mu, "sigma": sigma},
        "frozen": frozen,
        "loglik": ll,
        "aic": aic(ll, k),
        "bic": bic(ll, k, len(data)),
        "ks_stat": _ks_test(frozen, data)[0],
        "ks_p": _ks_test(frozen, data)[1],
    }


def fit_lognorm_2p(data: np.ndarray) -> Dict:
    # 2-parameter lognormal: loc=0
    # Scipy parameterization: s (shape=sigma), loc, scale=exp(mu)
    # Require positive data; filter out <=0
    pos = data[data > 0]
    if len(pos) < max(10, int(0.2 * len(data))):
        raise ValueError("Not enough positive data for 2-parameter lognormal (loc=0).")
    s, loc, scale = stats.lognorm.fit(pos, floc=0)
    mu = math.log(scale)
    frozen = stats.lognorm(s=s, loc=0, scale=scale)
    ll = _loglik_from_pdf(frozen, pos)
    k = 2  # mu, sigma
    return {
        "name": "Lognormal 2P (mu, sigma; loc=0)",
        "params": {"mu": mu, "sigma": s},
        "frozen": frozen,
        "loglik": ll,
        "aic": aic(ll, k),
        "bic": bic(ll, k, len(pos)),
        "ks_stat": _ks_test(frozen, pos)[0],
        "ks_p": _ks_test(frozen, pos)[1],
        "_n_used": len(pos),
    }


def fit_lognorm_3p(data: np.ndarray) -> Dict:
    s, loc, scale = stats.lognorm.fit(data)
    mu = math.log(scale)
    frozen = stats.lognorm(s=s, loc=loc, scale=scale)
    ll = _loglik_from_pdf(frozen, data)
    k = 3  # mu, sigma, loc
    return {
        "name": "Lognormal 3P (mu, sigma, loc)",
        "params": {"mu": mu, "sigma": s, "loc": loc},
        "frozen": frozen,
        "loglik": ll,
        "aic": aic(ll, k),
        "bic": bic(ll, k, len(data)),
        "ks_stat": _ks_test(frozen, data)[0],
        "ks_p": _ks_test(frozen, data)[1],
    }


def fit_weibull_2p(data: np.ndarray) -> Dict:
    # Weibull 2P = weibull_min with loc=0
    c, loc, scale = stats.weibull_min.fit(data, floc=0)
    frozen = stats.weibull_min(c=c, loc=0, scale=scale)
    ll = _loglik_from_pdf(frozen, data)
    k = 2  # shape, scale
    return {
        "name": "Weibull 2P (shape, scale; loc=0)",
        "params": {"shape": c, "scale": scale},
        "frozen": frozen,
        "loglik": ll,
        "aic": aic(ll, k),
        "bic": bic(ll, k, len(data)),
        "ks_stat": _ks_test(frozen, data)[0],
        "ks_p": _ks_test(frozen, data)[1],
    }


def fit_weibull_3p(data: np.ndarray) -> Dict:
    c, loc, scale = stats.weibull_min.fit(data)
    frozen = stats.weibull_min(c=c, loc=loc, scale=scale)
    ll = _loglik_from_pdf(frozen, data)
    k = 3  # shape, scale, loc
    return {
        "name": "Weibull 3P (shape, scale, loc)",
        "params": {"shape": c, "scale": scale, "loc": loc},
        "frozen": frozen,
        "loglik": ll,
        "aic": aic(ll, k),
        "bic": bic(ll, k, len(data)),
        "ks_stat": _ks_test(frozen, data)[0],
        "ks_p": _ks_test(frozen, data)[1],
    }


def fit_gamma_2p(data: np.ndarray) -> Dict:
    # Gamma 2P with loc=0 (shape k, scale theta)
    a, loc, scale = stats.gamma.fit(data, floc=0)
    frozen = stats.gamma(a=a, loc=0, scale=scale)
    ll = _loglik_from_pdf(frozen, data)
    k = 2
    return {
        "name": "Gamma 2P (shape, scale; loc=0)",
        "params": {"shape": a, "scale": scale},
        "frozen": frozen,
        "loglik": ll,
        "aic": aic(ll, k),
        "bic": bic(ll, k, len(data)),
        "ks_stat": _ks_test(frozen, data)[0],
        "ks_p": _ks_test(frozen, data)[1],
    }

def fit_gamma_3p(data: np.ndarray) -> Dict:
    # Gamma 3P: shape (a), scale (theta), and location (loc)
    a, loc, scale = stats.gamma.fit(data)
    frozen = stats.gamma(a=a, loc=loc, scale=scale)
    ll = np.sum(frozen.logpdf(data))
    k = 3 # shape, scale, loc
    return {
        "name": "Gamma 3P (shape, scale, loc)",
        "params": {"shape": a, "scale": scale, "loc": loc},
        "frozen": frozen,
        "loglik": ll,
        "aic": 2 * k - 2 * ll,
        "bic": math.log(len(data)) * k - 2 * ll,
        "ks_stat": stats.kstest(data, frozen.cdf)[0],
        "ks_p": stats.kstest(data, frozen.cdf)[1],
        }


def fit_expon_1p(data: np.ndarray) -> Dict:
    # Exponential 1P (lambda=1/scale) with loc=0
    loc, scale = stats.expon.fit(data, floc=0)
    frozen = stats.expon(loc=0, scale=scale)
    ll = _loglik_from_pdf(frozen, data)
    k = 1
    return {
        "name": "Exponential 1P (scale; loc=0)",
        "params": {"scale": scale, "rate": 1.0 / scale if scale > 0 else np.nan},
        "frozen": frozen,
        "loglik": ll,
        "aic": aic(ll, k),
        "bic": bic(ll, k, len(data)),
        "ks_stat": _ks_test(frozen, data)[0],
        "ks_p": _ks_test(frozen, data)[1],
    }


def build_percentile_table(fits_df: pd.DataFrame, data: np.ndarray, percentiles: List[float], empirical_label: str) -> pd.DataFrame:
    rows = []
    # Empirical percentiles
    try:
        emp = np.percentile(data, percentiles)
        rows.append({"Model": empirical_label, **{f"p{int(p)}": v for p, v in zip(percentiles, emp)}})
    except Exception:
        pass

    # Fitted models percentiles
    for i in range(len(fits_df)):
        fr = fits_df.iloc[i]["_frozen"]
        name = fits_df.iloc[i]["Model"]
        if fr is None:
            continue
        try:
            qs = fr.ppf(np.array(percentiles) / 100.0)
            rows.append({"Model": name, **{f"p{int(p)}": v for p, v in zip(percentiles, qs)}})
        except Exception:
            continue

    if not rows:
        return pd.DataFrame()
    dfp = pd.DataFrame(rows)
    # Order columns: Model then pX ascending
    cols = ["Model"] + [f"p{int(p)}" for p in percentiles]
    dfp = dfp.reindex(columns=cols)
    return dfp


FITTERS = [
    fit_normal,
    fit_lognorm_2p,
    fit_lognorm_3p,
    fit_weibull_2p,
    fit_weibull_3p,
    fit_gamma_2p,
    fit_gamma_3p,
    fit_expon_1p,
]


def run_fit_all(data: np.ndarray, enabled: Dict[str, bool]) -> pd.DataFrame:
    rows = []
    for fitter in FITTERS:
        name = fitter.__name__.replace("fit_", "").replace("_", " ").title()
        try:
            if not enabled.get(fitter.__name__, True):
                continue
            res = fitter(data)
            row = {
                "Model": res["name"],
                "loglik": res["loglik"],
                "AIC": res["aic"],
                "BIC": res["bic"],
                "KS": res["ks_stat"],
                "KS p": res["ks_p"],
                "_frozen": res["frozen"],
                "_params": res["params"],
            }
            rows.append(row)
        except Exception as e:
            rows.append({
                "Model": f"{fitter.__name__}",
                "loglik": np.nan,
                "AIC": np.inf,
                "BIC": np.inf,
                "KS": np.nan,
                "KS p": np.nan,
                "_frozen": None,
                "_params": {"error": str(e)},
            })
    if not rows:
        return pd.DataFrame()
    df = pd.DataFrame(rows)
    df = df.sort_values(["AIC", "BIC"], ascending=[True, True]).reset_index(drop=True)
    return df


def plot_with_fits(data: np.ndarray, fits_df: pd.DataFrame, bins: int = 30, title: str = ""):
    """Generic plotting for the *dataset* tab: histogram (density) + top-N PDFs.
    This function is intentionally agnostic of external variables (e.g., 'expanded', 'percentiles').
    """
    fig = go.Figure()
    # Histogram as density
    fig.add_histogram(x=data, nbinsx=bins, histnorm='probability density', name='Data', opacity=0.5)

    # Overlay PDFs for top-N models
    if len(data) > 0 and not fits_df.empty:
        x = np.linspace(np.min(data), np.max(data), 400)
        top_k = min(5, len(fits_df))
        for i in range(top_k):
            fr = fits_df.iloc[i]["_frozen"]
            label = fits_df.iloc[i]["Model"]
            if fr is None:
                continue
            y = fr.pdf(x)
            fig.add_trace(go.Scatter(x=x, y=y, mode='lines', name=label))

    fig.update_layout(
        title=title,
        xaxis_title="Value",
        yaxis_title="Density",
        legend_title="Fits",
    )
    st.plotly_chart(fig, use_container_width=True)


# ---------------------------------------------
# UI
# ---------------------------------------------

st.title("Best-Fit Probability Distributions")

with st.expander("About this app", expanded=False):
    st.markdown(textwrap.dedent(
        """
        This app estimates distribution parameters via Maximum Likelihood and ranks candidates by AIC.
        It supports **Normal**, **Lognormal (2P & 3P)**, **Weibull (2P & 3P)**, **Gamma (2P)**, and **Exponential (1P)**.

        **Notes**
        - The KS p-value is approximate because parameters are estimated from the data.
        - Lognormal 2P and Exponential 1P assume `loc=0` and require positive data.
        - For histogram input, the app accepts either `(value,count)` / `(bin,count)` or `(left,right,count)`.
        """
    ))

# Distribution toggles
with st.sidebar:
    st.header("Distributions")
    enabled = {}
    for f in FITTERS:
        enabled[f.__name__] = st.checkbox(f.__name__.replace("fit_", "").replace("_", " ").title(), True)

    st.divider()
    st.header("Chart")
    default_bins = 30
    bins = st.number_input("Histogram bins (dataset tab)", min_value=5, max_value=200, value=default_bins, step=1)
    st.caption("Histogram tab uses provided binning; this setting applies to the dataset tab only.")

    st.divider()
    st.header("Percentiles table")
    pct_text = st.text_input("Percentiles (comma-separated, 0-100)", value="1,5,10,25,50,75,90,95,99")
    percentiles = _parse_percentiles(pct_text)


TAB1, TAB2 = st.tabs(["Best fit – dataset", "Best fit – histogram"])

# ---------------------------------------------
# Tab 1: Dataset
# ---------------------------------------------
with TAB1:
    st.subheader("1) Upload or paste a numeric dataset")

    c1, c2 = st.columns([1, 1])
    with c1:
        file = st.file_uploader("Upload CSV/TXT (one numeric column or a column named e.g. value)", type=["csv", "txt", "data"], key="ds_file")
        sep = st.text_input("CSV separator", value=",", help="Try ';' or whitespace if parsing fails.")
        dec = st.text_input("Decimal symbol", value=".")

    with c2:
        st.markdown("Or paste values (one per line or comma/space separated):")
        txt = st.text_area("Paste data", value="", height=140)
        demo = st.checkbox("Use demo data (Weibull 2P)", value=False)

    data = None
    if demo:
        rng = np.random.default_rng(42)
        sample = stats.weibull_min.rvs(c=1.6, scale=50, size=1000, random_state=rng)
        data = pd.Series(sample)
    elif file is not None:
        try:
            data = read_numeric_series(file.read(), decimal=dec, sep=sep)
        except Exception as e:
            st.error(f"Failed to parse file: {e}")
    elif txt.strip():
        # Parse manual input
        raw = txt.replace("\n", " ").replace(";", " ")
        parts = [p for p in raw.replace(",", " ").split(" ") if p.strip()]
        vals = pd.to_numeric(pd.Series(parts), errors="coerce").dropna()
        data = vals

    if data is None or data.empty:
        st.info("Upload or paste data to begin.")
    else:
        st.write(f"Loaded **{len(data)}** observations. After cleaning NaNs: **{len(data.dropna())}**")
        arr = data.dropna().values.astype(float)
        if len(arr) < 5:
            st.warning("Not enough data to fit reliably (need at least 5).")
        else:
            fits_df = run_fit_all(arr, enabled)
            if fits_df.empty:
                st.error("No fits were produced.")
            else:
                # Display summary
                show = fits_df[["Model", "AIC", "BIC", "KS", "KS p"]].copy()
                st.dataframe(show, use_container_width=True)

                # Best model details
                best = fits_df.iloc[0]
                st.success(f"Best by AIC: **{best['Model']}**")
                st.json(best["_params"], expanded=False)

                plot_with_fits(arr, fits_df, bins=bins, title="Dataset with fitted PDFs (top 5 by AIC)")

                # Percentiles table (empirical + fitted)
                dfp = build_percentile_table(fits_df, arr, percentiles, empirical_label="Empirical (data)")
                if not dfp.empty:
                    st.markdown("**Percentiles (values):**")
                    st.dataframe(dfp, use_container_width=True)

# ---------------------------------------------
# Tab 2: Histogram
# ---------------------------------------------
with TAB2:
    st.subheader("2) Upload a histogram file")
    st.markdown("Accepted columns: **(value,count)** or **(bin,count)** or **(left,right,count)**")

    c1, c2 = st.columns([1, 1])
    with c1:
        hfile = st.file_uploader("Upload histogram CSV/TXT", type=["csv", "txt", "data"], key="hist_file")
    with c2:
        st.markdown("Or paste small histogram (CSV):")
        htxt = st.text_area("Example: value,count\\n0.5,10\\n1.5,15\\n2.5,8", value="", height=140)

    if hfile is None and not htxt.strip():
        st.info("Upload or paste a histogram to begin.")
    else:
        try:
            if hfile is not None:
                hdf = read_histogram(hfile.read())
            else:
                hdf = read_histogram(htxt.encode())
        except Exception as e:
            st.error(f"Failed to parse histogram: {e}")
            hdf = None

        if hdf is not None and not hdf.empty:
            st.dataframe(hdf, use_container_width=True)
            total_count = int(hdf["count"].sum())
            st.write(f"Total count = **{total_count}**; bins = **{len(hdf)}**")

            # Convert to pseudo-sample (repeat midpoints). If huge, downsample proportionally.
            MAX_POINTS = 200_000
            mids = hdf["mid"].values
            counts = hdf["count"].astype(int).values.clip(min=0)
            if total_count <= MAX_POINTS:
                expanded = np.repeat(mids, counts)
            else:
                # Proportional downsampling
                scale = MAX_POINTS / total_count
                downsized_counts = np.maximum((counts * scale).astype(int), 1)
                expanded = np.repeat(mids, downsized_counts)
                st.warning("Histogram very large; using proportional downsampling for fitting and KS.")

            # --- Build & download dataset from histogram ---
            st.markdown("### Create dataset from histogram")
            make_full = st.radio(
                "Dataset size",
                ["Use fitting sample (may be downsampled)", "Full expansion (can be very large)"],
                index=0,
                help="Fitting sample uses the same array used for fitting/KS (downsampled if needed). Full expansion repeats each bin midpoint by its count."
            )
            max_full = st.number_input("Safety cap for full expansion (max points)", min_value=10000, max_value=10000000, value=2000000, step=10000)
            fname_base = st.text_input("Base filename (without extension)", value="histogram_dataset")

            if make_full.startswith("Full"):
                # Respect safety cap
                if total_count > max_full:
                    st.warning(f"Full expansion would create {total_count:,} points; capping to {max_full:,}.")
                    # Proportional cap
                    scale = max_full / total_count
                    capped_counts = np.maximum((counts * scale).astype(int), 1)
                    dataset = np.repeat(mids, capped_counts)
                else:
                    dataset = np.repeat(mids, counts)
            else:
                dataset = expanded

            ds_series = pd.Series(dataset.astype(float), name="value")
            c1, c2, c3 = st.columns(3)
            with c1:
                st.download_button(
                    label="Download CSV",
                    data=_series_to_csv_bytes(ds_series),
                    file_name=f"{fname_base}.csv",
                    mime="text/csv",
                )
            with c2:
                st.download_button(
                    label="Download TXT",
                    data=_series_to_txt_bytes(ds_series),
                    file_name=f"{fname_base}.txt",
                    mime="text/plain",
                )
            with c3:
                try:
                    xlsx_bytes = _series_to_xlsx_bytes(ds_series)
                    st.download_button(
                        label="Download Excel (.xlsx)",
                        data=xlsx_bytes,
                        file_name=f"{fname_base}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    )
                except Exception as e:
                    st.info(f"Excel writer not available ({e}); use CSV/TXT instead.")

            if len(expanded) < 5:
                st.warning("Histogram implies too few points to fit (need at least 5).")
            else:
                fits_df = run_fit_all(expanded.astype(float), enabled)
                if fits_df.empty:
                    st.error("No fits were produced.")
                else:
                    show = fits_df[["Model", "AIC", "BIC", "KS", "KS p"]].copy()
                    st.dataframe(show, use_container_width=True)

                    best = fits_df.iloc[0]
                    st.success(f"Best by AIC: **{best['Model']}**")
                    st.json(best["_params"], expanded=False)

                    # Plot: binned bars + PDFs
                    fig = go.Figure()
                    fig.add_bar(x=hdf["mid"], y=hdf["count"], width=hdf["width"], name="Histogram counts", opacity=0.6)

                    x_low = hdf["left"].min()
                    x_high = hdf["right"].max()
                    x = np.linspace(x_low, x_high, 400)
                    # Convert PDFs to expected counts per bin width * total_count
                    top_k = min(5, len(fits_df))
                    for i in range(top_k):
                        fr = fits_df.iloc[i]["_frozen"]
                        label = fits_df.iloc[i]["Model"]
                        if fr is None:
                            continue
                        y_density = fr.pdf(x)
                        # For display, scale density to counts roughly by average bin width * total_count
                        avg_w = np.average(hdf["width"].values, weights=hdf["count"].values)
                        y_counts = y_density * avg_w * total_count
                        fig.add_trace(go.Scatter(x=x, y=y_counts, mode='lines', name=label))

                    fig.update_layout(
                        title="Histogram with fitted PDFs (scaled to counts)",
                        xaxis_title="Value",
                        yaxis_title="Counts (approx.)",
                        legend_title="Fits",
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    # Percentiles table for histogram (empirical from expanded + fitted)
                    dfp = build_percentile_table(
                        fits_df,
                        expanded.astype(float),
                        percentiles,
                        empirical_label="Empirical (histogram)",
                    )
                    if not dfp.empty:
                        st.markdown("**Percentiles (values):**")
                        st.dataframe(dfp, use_container_width=True)


# Footer
st.caption("Built with Streamlit, SciPy, NumPy, Pandas, and Plotly. © 2025")
